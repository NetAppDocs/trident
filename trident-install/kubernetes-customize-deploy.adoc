---
sidebar: sidebar
permalink: trident-get-started/kubernetes-customize-deploy.html
keywords: deploy, trident, methods, operator, tridentctl, helm, attributes, customize, tridentorchestrator, smb, windows
summary: The Trident operator enables you to customize the manner in which Trident is installed by using the attributes in the `TridentOrchestrator` spec.
---

= Customize Trident operator installation
:hardbreaks:
:icons: font
:imagesdir: ../media/

[.lead]
The Trident operator allows you to customize Trident installation using the attributes in the `TridentOrchestrator` spec. If you want to customize the installation beyond what `TridentOrchestrator` arguments allow, consider using `tridentctl` to generate custom YAML manifests to modify as needed.

== Understanding controller pods and node pods
Trident runs as a single controller pod and a node pod on each worker node in the cluster. The node pod must be running on any host where you want to potentially mount a Trident volume. 

Kubernetes link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/[node selectors^] and link:https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/[tolerations and taints^] are used to constrain a pod to run on a specific or preferred node. Using the`ControllerPlugin` and `NodePlugin`, you can specify constraints and overrides.

* The controller plugin handles volume provisioning and management, such as snapshots and resizing. 
* The node plugin handles attaching the storage to the node.

== Configuration options
WARNING: `spec.namespace` is specified in `TridentOrchestrator` to signify the namespace where Trident is installed. This parameter *cannot be updated after Trident is installed*. Attempting to do so causes the `TridentOrchestrator` status to change to `Failed`. Trident is not intended to be migrated across namespaces.

This table details `TridentOrchestrator` attributes.
[cols="1,2,1",options="header"]
|===
|Parameter |Description |Default
|`namespace` |Namespace to install Trident in |`"default"`

|`debug` |Enable debugging for Trident |`false`

|`enableForceDetach` |`ontap-san`, `ontap-san-economy`, `ontap-nas`, and `ontap-nas-economy` only. 

Works with Kubernetes Non-Graceful Node Shutdown (NGNS) to grant cluster administrators ability to safely migrate workloads with mounted volumes to new nodes should a node become unhealthy. 

For information about force detach and automated failover, see https://review.docs.netapp.com/us-en/trident_rn-trident-2510/trident-get-started/kubernetes-customize-deploy.html#details-about-force-detach[Details about force detach^].|`false`

|`windows` | Setting to `true` enables installation on Windows worker nodes. | `false`

|`cloudProvider` a| Set to `"Azure"` when using managed identities or a cloud identity on an AKS cluster. 
Set to `"AWS"` when using a cloud identity on an EKS cluster.
Set to `"GCP"` when using a cloud identity on a GKE cluster. |`""` 

|`cloudIdentity` a|Set to workload identity ("azure.workload.identity/client-id: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx") when using cloud identity on an AKS cluster. 
Set to AWS IAM role ("'eks.amazonaws.com/role-arn: arn:aws:iam::123456:role/trident-role'") when using cloud identity on an EKS cluster.
Set to cloud identity ("'iam.gke.io/gcp-service-account: xxxx@mygcpproject.iam.gserviceaccount.com'") when using cloud identity on a GKE cluster.|`""` 

|`IPv6` |Install Trident over IPv6 |false

|`k8sTimeout` a|Timeout for Kubernetes operations.

NOTE: The `k8sTimeout` parameter is applicable only for Trident installation.  |`180sec`

|`silenceAutosupport` |Don't send autosupport bundles to NetApp
automatically |`false`

|`autosupportImage` |The container image for Autosupport Telemetry
|`"netapp/trident-autosupport:25.06"`

|`autosupportProxy` |The address/port of a proxy for sending Autosupport
Telemetry |`"http://proxy.example.com:8888"`

|`uninstall` |A flag used to uninstall Trident |`false`

|`logFormat` |Trident logging format to be used [text,json] |`"text"`

|`tridentImage` |Trident image to install |`"netapp/trident:25.06"`

|`imageRegistry` |Path to internal registry, of the format
`<registry FQDN>[:port][/subpath]` |`"registry.k8s.io"`

|`kubeletDir` |Path to the kubelet directory on the host |`"/var/lib/kubelet"`

|`wipeout` |A list of resources to delete to perform a complete removal of
Trident |

|`imagePullSecrets` |Secrets to pull images from an internal registry |

|`imagePullPolicy` | Sets the image pull policy for the the Trident operator. Valid values are:

`Always` to always pull the image.

`IfNotPresent` to pull the image only if it does not already exist on the node.

`Never` to never pull the image. |`IfNotPresent`

|`controllerPluginNodeSelector` |Additional node selectors for pods.	Follows same format as `pod.spec.nodeSelector`. |No default; optional

|`controllerPluginTolerations` |Overrides Kubernetes tolerations for pods. Follows the same format as `pod.spec.Tolerations`. |No default; optional

|`nodePluginNodeSelector` |Additional node selectors for pods. Follows same format as `pod.spec.nodeSelector`. |No default; optional

|`nodePluginTolerations` |Overrides Kubernetes tolerations for pods. Follows the same format as `pod.spec.Tolerations`. |No default; optional

|`nodePrep`
a|Enables Trident to prepare the nodes of the Kubernetes cluster to manage volumes using the specified data storage protocol. 
*Currently, `iscsi` is the only value supported.*

NOTE: Beginning with OpenShift 4.19, the minimum Trident version supported for this feature is 25.06.1.
|

|`k8sAPIQPS` a|The queries per second (QPS) limit used by the controller while communicating with the Kubernetes API server. The Burst value is set automatically based on the QPS value.|`100`; optional

|`enableConcurrency` a|Enables concurrent Trident controller operations for improved throughput.

NOTE: *Tech Preview*: This feature is experimental in NetApp Trident 25.06 and currently supports limited parallel workflows with the ONTAP-SAN driver (iSCSI and FCP protocols).|false


|===
[NOTE] 
For more information on formatting pod parameters, refer to link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/[Assigning Pods to Nodes^].

=== Details about force detach
Force detach is available for `ontap-san`, `ontap-san-economy`, `ontap-nas`, and `ontap-nas-economy` only. Before enabling force detach, non-graceful node shutdown (NGNS) must be enabled on the Kubernetes cluster. NGNS is enabled by default for Kubernetes 1.28 and above. For more information, refer to link:https://kubernetes.io/docs/concepts/cluster-administration/node-shutdown/#non-graceful-node-shutdown[Kubernetes: Non Graceful node shutdown^]. 

NOTE: When using the `ontap-nas` or `ontap-nas-economy` driver, you need to set the `autoExportPolicy` parameter in the backend configuration to `true` so that Trident can restrict access from the Kubernetes node with the taint applied using managed export policies.

WARNING: Because Trident relies on Kubernetes NGNS, do not remove `out-of-service` taints from an unhealthy node until all non-tolerable workloads are rescheduled. Recklessly applying or removing the taint can jeopardize backend data protection.  

When the Kubernetes cluster administrator has applied the `node.kubernetes.io/out-of-service=nodeshutdown:NoExecute` taint to the node and `enableForceDetach` is set to `true`, Trident will determine the node status and:

. Cease backend I/O access for volumes mounted to that node.
. Mark the Trident node object as `dirty` (not safe for new publications).
+
NOTE: The Trident controller will reject new publish volume requests until the node is re-qualified (after having been marked as `dirty`) by the Trident node pod. Any workloads scheduled with a mounted PVC (even after the cluster node is healthy and ready) will be not be accepted until Trident can verify the node `clean` (safe for new publications).

When node health is restored and the taint is removed, Trident will:

. Identify and clean stale published paths on the node.
. If the node is in a `cleanable` state (the out-of-service taint has been removed and the node is in `Ready` state) and all stale, published paths are clean, Trident will readmit the node as `clean` and allow new published volumes to the node.

=== Details about automated failover

You can automates the force-detach process through integration with node health check (NHC) operator. When a node failure occurs, NHC triggers Trident node remediation (TNR) and force-detach automatically by creating a TridentNodeRemediation  CR in Trident's namespace defining the failed node. TNR is created only upon node failure, and removed by NHC once the node comes back online or the node is deleted.

*Pods and volume supported by automated-failover*: 

All volumes/PVCs supported by force-detach are supported by automated-failover:

* NAS, and NAS-economy volumes using auto-export policies.
* SAN, and SAN-economy volumes.

*Prerequisites*:

* Ensure that force detach is enabled before enabling automated-failover. For more information, refer to <<Details about force detach>>.
* Install node health check (NHC) in the Kubernetes cluster.
** link:https://sdk.operatorframework.io/docs/installation/[Install operator-sdk]. 
** Install Operator Lifecycle Manager (OLM) in the cluster if not already installed.
** Install Node-Healthcheck-Operator: `kubectl create -f https://operatorhub.io/install/node-healthcheck-operator.yam`

See link:https://www.redhat.com/en/blog/node-health-check-operator[Node Health Check Operator^] for more information.

.Steps to enable automated-failover
. Create a NodeHealthCheck (NHC) CR in the Trident namespace to monitor all nodes in the cluster. Example:
+
[source,yaml]
----
apiVersion: remediation.medik8s.io/v1alpha1
kind: NodeHealthCheck
metadata:
  name: <CR name>
spec:
  selector:
    matchExpressions:
      - key: node-role.kubernetes.io/control-plane
        operator: DoesNotExist
      - key: node-role.kubernetes.io/master
        operator: DoesNotExist
  remediationTemplate:
    apiVersion: trident.netapp.io/v1
    kind: TridentNodeRemediationTemplate
    namespace: <Trident installation namespace>
    name: trident-node-remediation-template
  minHealthy: 0 # Trigger force-detach upon one or more node failures
  unhealthyConditions:
    - type: Ready
      status: "False"
      duration: 0s
    - type: Ready
      status: Unknown
      duration: 0s
----
. Apply the node health check CR in the `trident` namespace.
+
`kubectl apply -f <nhc-cr-file>.yaml -n <trident-namespace>`

The above CR is configured to watch K8s worker nodes for node conditions Ready: false and Unknown. Automated-Failover will be triggered upon a node going into Ready: false, or Ready: Unknown state.

The `unhealthyConditions` in the CR uses a 0 second grace period. This causes automated-failover to trigger immediately upon K8s setting node condition Ready: false, which is set after K8s loses the heartbeat from a node. K8s has a default of 40sec wait after the last heartbeat before setting Ready: false. This grace-period can be customized in K8s deployment options.

For additional configuration options, refer to link:https://github.com/medik8s/node-healthcheck-operator/blob/main/docs/configuration.md[can be found in Node-Healthcheck-Operator documentation^].

*Additional Setup Information*:

When Trident is installed with force-detach enabled, two additional resources are created in the Trident namespace to facilitate integration with NHC.

*TridentNodeRemediationTemplate (TNRT)*:

The TNRT serves as a template for the NHC controller, which uses TNRT to generate TNR resources as needed.

[source,yaml]
----
apiVersion: trident.netapp.io/v1
kind: TridentNodeRemediationTemplate
metadata:
  name: trident-node-remediation-template
  namespace: trident
spec:
  template:
    spec: {}
----

*ClusterRole*:

A cluster role is also added during the installation when force-detach is enabled. This gives NHC permissions to TNRs in the Trident namespace.

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    rbac.ext-remediation/aggregate-to-ext-remediation: "true"
  name: tridentnoderemediation-access
rules:
- apiGroups:
  - trident.netapp.io
  resources:
  - tridentnoderemediationtemplates
  - tridentnoderemediations
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
----

*TNR states*:
Use the following commands to view the status of TNRs:
`kubectl get tnr <name> -n <trident-namespace>`

TNRs can be in one of the following states:

* _Remediating_:

* Cease backend I/O access for volumes supported by force-detach mounted to that node.
* The Trident node object is marked dirty (not safe for new publications).
* Remove pods and volume attachments from the node

* _NodeRecoveryPending_:

* The controller is waiting for the node to come back online.
* Once the node is online, publish-enforcement will ensure the node is clean and ready for new volume publications.
* If the node is deleted from K8s, the TNR controller will remove the TNR and cease reconciliation.

* _Succeeded_:
* All remediation and node recovery steps completed successfully. The node is clean and ready for new volume publications.

* _Failed_:
* Unrecoverable error. Error reasons are set in the status.message field of the CR.


*K8s Cluster Upgrades and Maintenance*:

To prevent any failovers, pause automated-failover during K8s maintenance or upgrades, where the nodes are expected to go down or reboot. You can pause the NHC CR (described above) by patching its CR:

`kubectl patch NodeHealthCheck <cr-name> --patch '{"spec":{"pauseRequests":["<description-for-reason-of-pause>"]}}' --type=merge`

This pauses the automated-failover. The pauseRequests can be removed from the spec after the maintenance is complete to re-enable automated-failover.

*Failed Node Pod Removal Process*:

Automated-failover selects which workloads to remove from the failed node. When a TNR is created, the TNR controller marks the node as dirty, preventing any new volume publications and begins removing force-detach supported pods and their volume attachments.

*Default Behavior*:

* Pods using volumes all supported by force-detach are removed from the failed node. Kubernetes will reschedule these onto a healthy node. 
* Pods using a volume not supported by force-detach, including non-Trident volumes, will not be removed from the failed node.
* Stateless pods (no PVCs) will not be removed from the failed node.

*Overriding Pod Removal Behavior*:

Pod removal behavior can be customized using a pod annotation: `trident.netapp.io/podRemediationPolicy`: [retain, delete]. These annotations are examined and used when a failover occurs. 
Apply annotations to the Kubernetes deployment/replicaset pod spec to prevent the annotation from disappearing after a failover:
* `retain` - Pod WILL NOT be removed from the failed node during an automated-failover. 
* `delete` - Pod WILL be removed from the failed node during an automated-failover.

These annotations can be applied to any pod.

[WARNING] 
====
* I/O operations will be blocked only on failed nodes for volumes that support force-detach.
* For volumes that do not support force-detach, there is a risk of data corruption and multi-attach issues.
====

*Limitations*:

* I/O operations will only be prevented on the failed nodes for volumes supported by force-detach. Only pods using volumes/PVCs supported by force-detach will automatically be removed. 
Automatic-failover and force-detach run inside the trident-controller pod. If the node hosting trident-controller fails, automated-failover will be delayed until K8s moves the pod to a healthy node.

*Integrating Custom Node Health Check Solutions*:
You can replace Node-Healthcheck-Operator with alternative node failure detection tools to trigger automatic-failover. 
To ensure compatibility with the automated failover mechanism, your custom solution should:

* Create a TNR when a node failure is detected, using the failed nodeâ€™s name as the TNR CR name.
* Delete the TNR when the node has recovered and the TNR is in the Succeeded state.

== Sample configurations
You can use the attributes in <<Configuration options>> when defining `TridentOrchestrator` to customize your installation. 

.Basic custom configuration
[%collapsible%closed]
====
This example, created after running the `cat deploy/crds/tridentorchestrator_cr_imagepullsecrets.yaml` command, represents a basic custom installation:
[source,yaml]
----
apiVersion: trident.netapp.io/v1
kind: TridentOrchestrator
metadata:
  name: trident
spec:
  debug: true
  namespace: trident
  imagePullSecrets:
  - thisisasecret
----

====

.Node selectors
[%collapsible%closed]
====

This example installs Trident with node selectors.
[source,yaml]
----
apiVersion: trident.netapp.io/v1
kind: TridentOrchestrator
metadata:
  name: trident
spec:
  debug: true
  namespace: trident
  controllerPluginNodeSelector:
    nodetype: master
  nodePluginNodeSelector:
    storage: netapp
----
====

.Windows worker nodes
[%collapsible%closed]
====
This example, created after running the `cat deploy/crds/tridentorchestrator_cr.yaml` command, installs Trident on a Windows worker node.
[source,yaml]
----
apiVersion: trident.netapp.io/v1
kind: TridentOrchestrator
metadata:
  name: trident
spec:
  debug: true
  namespace: trident
  windows: true
----
====

.Managed identities on an AKS cluster
[%collapsible%closed]
====
This example installs Trident to enable managed identities on an AKS cluster. 
[source,yaml]
----
apiVersion: trident.netapp.io/v1
kind: TridentOrchestrator
metadata:
  name: trident
spec:
  debug: true
  namespace: trident
  cloudProvider: "Azure"
----
====

.Cloud identity on an AKS cluster
[%collapsible%closed]
====
This example installs Trident for use with a cloud identity on an AKS cluster. 
[source,yaml]
----
apiVersion: trident.netapp.io/v1
kind: TridentOrchestrator
metadata:
  name: trident
spec:
  debug: true
  namespace: trident
  cloudProvider: "Azure"
  cloudIdentity: 'azure.workload.identity/client-id: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx'
 
----
====

.Cloud identity on an EKS cluster
[%collapsible%closed]
====
This example installs Trident for use with a cloud identity on an AKS cluster. 
[source,yaml]
----
apiVersion: trident.netapp.io/v1
kind: TridentOrchestrator
metadata:
  name: trident
spec:
  debug: true
  namespace: trident
  cloudProvider: "AWS"
  cloudIdentity: "'eks.amazonaws.com/role-arn: arn:aws:iam::123456:role/trident-role'"
----
====

.Cloud identity for GKE
[%collapsible%closed]
====

This example installs Trident for use with a cloud identity on a GKE cluster. 
[source,yaml]
----
apiVersion: trident.netapp.io/v1
kind: TridentBackendConfig
metadata:
  name: backend-tbc-gcp-gcnv
spec:
  version: 1
  storageDriverName: google-cloud-netapp-volumes 
  projectNumber: '012345678901'
  network: gcnv-network
  location: us-west2
  serviceLevel: Premium
  storagePool: pool-premium1
----
====