---
sidebar: sidebar
permalink: trident-reference/k8s-cluster-arch.html
keywords: kubernetes, clusters, master, compute, architecture
summary: There are three primary Kubernetes cluster architectures. These accommodate various methods of high availability and recoverability of the cluster, its services, and the applications that are running. The Trident installation steps remain the same and are independent of the Kubernetes architecture.
---

= Kubernetes cluster architecture
:hardbreaks:
:icons: font
:imagesdir: ../media/

There are three primary Kubernetes cluster architectures. These accommodate various methods of high availability and recoverability of the cluster, its services, and the applications that are running. The Trident installation steps remain the same and are independent of the Kubernetes architecture.

Regardless of the architecture that you choose, it’s important to understand the ramifications to high availability, scalability, and serviceability of the component services. Be sure to consider the effect on the applications being hosted by the Kubernetes or OpenShift cluster.

== Single master, compute

This architecture is the easiest to deploy but does not provide high availability of the core management services. In the event the master node is unavailable, no interaction can happen with the cluster until, at a minimum, the Kubernetes API server is returned to service.

This architecture can be useful for testing, qualification, proof-of-concept, and other non-production uses, however it should *never* be used for production deployments.

A single node used to host both the master service and the workloads is a variant of this architecture. Using a single node Kubernetes cluster is useful when testing or experimenting with different concepts and capabilities. However, the limited scale and capacity make it unreasonable for more than very small tests.

NOTE: See the link:../trident-get-started/quickstart.html[Quick Start^] to learn the process for instantiating a single node Kubernetes cluster with Trident.

== Multiple master, compute

Having multiple master nodes ensures that services remain available if the master node(s) fail. Deploying with multiple masters is the minimum recommended configuration for most production clusters.

To facilitate the availability of master services, you should deployed them with odd numbers (for example, 3,5,7,9, and so on) so that quorum (master node majority) can be maintained if one or more masters fail. In the HA scenario, Kubernetes will maintain a copy of the `etcd` databases on each master, but hold elections for the control plane function leaders, `kube-controller-manager` and `kube-scheduler`, to avoid conflicts. The worker nodes can communicate with any master's API server through a load balancer.

Here is the multiple master architecture:

image::MultiMasterCluster2.png[Shows the multiple master Kubernetes architecture.]

*Pros*

* Provides highly-available master services, ensuring that the loss of up to (n/2) – 1 master nodes does not affect the cluster operations.

*Cons*

* More complex initial setup.

== Multiple master, `etcd`, compute

This architecture isolates the `etcd` cluster from the other master server services. It removes the workload from the master servers, enabling them to be sized smaller, and makes their scale out (or in) more simple.
Deploying a Kubernetes cluster by using this architecture adds a degree of complexity, however, it adds flexibility to the scale, support, and management of the `etcd` service used by Kubernetes.

Here is the multiple master, `etcd`, and compute architecture:

image::MultietcdCluster1.png[Shows the multiple master, etcd, and compute architecture.]

*Pros*

* Provides highly-available master services, ensuring that the loss of up to (n/2) – 1 master nodes does not affect the cluster operations.
* Isolating `etcd` from the other master services reduces the workload for master servers.
* Decoupling `etcd` from the masters makes `etcd` administration and protection easier. Independent management allows for different protection and scaling schemes.

*Cons*

* More complex initial setup.

== Red Hat OpenShift infrastructure architecture

Red Hat's OpenShift introduces the concept of https://docs.openshift.com/container-platform/3.11/admin_guide/manage_nodes.html#infrastructure-nodes[infrastructure nodes^]. These nodes host cluster services, such as log aggregation, metrics collection and reporting, container registry services, and overlay network management and routing.

Red Hat recommends a minimum of three infrastructure nodes for production deployments. This ensures that the services have resources available and are able to migrate in the event of host maintenance or failure.

This architecture enables the services, which are critical to the cluster, such as registry, overlay network routing, and others to be hosted on dedicated nodes. These dedicated nodes can have additional redundancy, different CPU/RAM requirements, and other low-level differences from the compute nodes. This also makes adding and removing compute nodes as needed easier, without needing to worry about core services being affected by a node being evacuated.

Here is the OpenShift infrastructure architecture:

image::MultiInfraCluster1.png[Shows the OpenShift infrastructure architecture.]

An additional option involves separating the master and `etcd` roles into different servers in the same way as can be done in Kubernetes. This results in having master, `etcd`, infrastructure, and compute node roles. See https://docs.openshift.com/container-platform/3.11/install/index.html[Red Hat documentation^] for more information, including examples of OpenShift node roles and potential deployment options.
